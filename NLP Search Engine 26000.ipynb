{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Simple Search Engine using NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will start by importing the required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs \n",
    "import re\n",
    "import numpy as np\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Let us load the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-20667282d85c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./wiki-26000'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\codecs.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 698\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    699\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\codecs.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size, chars, firstline)\u001b[0m\n\u001b[0;32m    499\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m                 \u001b[0mnewchars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecodedbytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mfirstline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text = codecs.open('./wiki-26000', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "starts = [match.span()[0] for match in re.finditer('\\n = [^=]', text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ii, start in enumerate(starts):\n",
    "    end = starts[ii+1] if ii+1 < len(starts) else len(text)\n",
    "    articles.append(text[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snippets = [' '.join(article[:300].split()) for article in articles]\n",
    " \n",
    "for snippet in snippets[:20]:\n",
    "    print(snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes:\n",
    "\n",
    "1. We are using the wiki-600 to begin with.\n",
    "2. All articles are in one file. Articles titles are formatted as follows: = Albert Einstein = . If it has two or more =      signs on both sides, then it's a subheading. The regex looks for article titles, and splits the text file.\n",
    "3. Then we calculate snippets, i.e. the first 300 characters for each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenising the article \n",
    "# Calculating Term Frequencies\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize               # <=== tokenizer \n",
    "from nltk.stem.porter import PorterStemmer            # <=== stemmer \n",
    "from nltk.corpus import stopwords as nltk_stopwords   # <=== stopwords\n",
    "STOPWORDS = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We start by initiating PorterStemmer, it is used to normalize words for e.g. Loves, Love, loving, loved all these words give the same context, but their multiple occurance gives more vectors and more computation is required. So its better to leave them\n",
    "2. Inside the function, we first lower case the words to have all the words in same patter\n",
    "3. Then we remove everything excecpt Alphanumerics\n",
    "4. We split and join to give a string again\n",
    "5. Then we remove the stopwords\n",
    "6. In the end we have cleaned dataset with only aplhanumerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "term_frequency = defaultdict(dict)\n",
    " \n",
    "def get_tokens(article):\n",
    "    article = article.lower()\n",
    "    article = re.sub(r'[^a-zA-Z0-9]', ' ', article)\n",
    "    article.split()\n",
    "    article = ''.join(article)\n",
    "    tokens = [ps.stem(tokens) for tokens in article.split() if tokens not in nltk_stopwords.words(\"english\")]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the index function, we first tokenize the given article, then we count the number of occurance of every token and store it in the term frequency dictionary\n",
    "\n",
    "Then using for loop we populate the term_frequency dictionary and it gives us term frequency of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def index(id, article):\n",
    "    tokens = get_tokens(article)\n",
    "    # TODO: calculate term frequencies and store in term_frequency[token][id]\n",
    "    for token in tokens:\n",
    "        term_frequency[token][id] = tokens.count(token)\n",
    " \n",
    "for ii, article in enumerate(articles):\n",
    "    if ii and ii % 10 == 0: print(ii, end=', ')\n",
    "    sys.stdout.flush()\n",
    "    index(ii, article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a random check of our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('term_frequency for \"einstein\"')\n",
    "print(term_frequency['einstein'])\n",
    "print(len(term_frequency['einstein']))\n",
    "# Expected output: {300: 1, 84: 5, 294: 1}\n",
    "# That is, articles[300] has token einstein 1 times, articles[5] has \n",
    "# token einstein 5 times, and articles[294] has token einstein 1 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to count **TFIDF**, here we are calling the **term_frequncy** for the particular word. And then counting the articles which contain the word(query). And using these values we are calculating the TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TFIDF(article_id, query):\n",
    "    tf = term_frequency[query][article_id]\n",
    "    total_articles = len(snippets)\n",
    "    desired_articles = len(term_frequency[query])\n",
    "    if desired_articles!=0:\n",
    "        tfidf = tf*np.log(total_articles/desired_articles)\n",
    "    else:\n",
    "        tfidf = tf*np.log(total_articles/1)\n",
    "    return tfidf\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the data in pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "## saving and loading\n",
    " \n",
    "import pickle\n",
    " \n",
    "def picklesave(obj, filename):\n",
    "    print('Saving .. ')\n",
    "    ff = open(filename, 'wb')\n",
    "    pickle.dump(obj, ff)\n",
    "    ff.close()\n",
    "    print('Done')\n",
    "    return True\n",
    " \n",
    "def pickleload(filename):\n",
    "    print('Loading .. ')\n",
    "    ff = open(filename, 'rb')\n",
    "    obj = pickle.load(ff)\n",
    "    ff.close()\n",
    "    print('Done')\n",
    "    return obj\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "picklesave([snippets, term_frequency], 'data-26000.pdata')\n",
    "snippets, term_frequency = pickleload('data-26000.pdata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking the articles for Search!\n",
    "It's time to write the final search function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sort(dictionary):\n",
    "    m= defaultdict(float)\n",
    "    for w in sorted(dictionary, key=dictionary.get, reverse=True):\n",
    "        m[w]=dictionary[w]\n",
    "    return m\n",
    "D = len(snippets)\n",
    "def search(query, nresults=10):\n",
    "    tokens = get_tokens(query)\n",
    "    scores = defaultdict(float)\n",
    "    for token in tokens:\n",
    "        for article, score in term_frequency[token].items():\n",
    "            scores[article] = TFIDF(article,token) \n",
    "    new_scores = sort(scores)\n",
    "    return new_scores# TODO: top nresults results\n",
    " \n",
    "def display_results(query, results):\n",
    "    print('You search for: \"%s\"' % query)\n",
    "    print('-'*100)\n",
    "    for result in results:\n",
    "        print(snippets[result])\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The interactive search bar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "## interactive\n",
    "while True:\n",
    "    clear_output()\n",
    "    query = input(\"Please enter the search query: \\nEnter q to quit\")\n",
    "    display_results(query, search(query))\n",
    "    if query=='q':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
